% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/estimation.R
\name{estimate_ridge}
\alias{estimate_ridge}
\title{Estimating a Single Model}
\usage{
estimate_ridge(
  Y,
  X,
  K,
  lambda,
  compute_kernel_terms = TRUE,
  converge_thres = 1e-04
)
}
\arguments{
\item{Y}{(matrix, n*1) The vector of response variable.}

\item{X}{(matrix, n*d_fix) The fixed effect matrix.}

\item{K}{(list of matrices) A nested list of kernel term matrices, 
corresponding to each kernel term specified in the formula for 
a base kernel function in kern_func_list.}

\item{lambda}{(numeric) A numeric string specifying the range of tuning parameter 
to be chosen. The lower limit of lambda must be above 0.}

\item{compute_kernel_terms}{(logic) Whether to computing effect for each individual terms.
If FALSE then only compute the overall effect.}

\item{converge_thres}{(numeric) The convergence threshold for computing kernel terms.}
}
\value{
\item{beta}{(matrix, d_fixed*1) Fixed effect estimates.}

\item{alpha}{(matrix, n*k_terms) Kernel effect estimates for each kernel term.}

\item{kern_term_mat}{(matrix, n*k_terms) Kernel effect for each kernel term.}

\item{A_list}{(list of length k_terms) Projection matrices for each kernel term.}

\item{proj_matrix}{(list of length 4) Estimated projection matrices, combined 
across kernel terms.}
}
\description{
Estimating projection matrices and parameter estimates for a single model.
}
\details{
For a single model, we can calculate the output of gaussian process
regression, the solution is given by \deqn{\hat{\beta}=[X^T(K+\lambda
I)^{-1}X]^{-1}X^T(K+\lambda I)^{-1}y} \deqn{\hat{\alpha}=(K+\lambda
I)^{-1}(y-\hat{\beta}X)}.
}
\examples{
kern_par <- data.frame(method = c("linear", "polynomial", "matern"), 
                       l = c(.5, 1, 1.5), p = 1:3, stringsAsFactors = FALSE)
# define kernel library
kern_func_list <- define_library(kern_par)
n <- 50
d <- 4
formula <- y ~ x1 + x2 + k(x3, x4)
data <- as.data.frame(matrix(
  rnorm(n * d),
  ncol = d,
  dimnames = list(NULL, paste0("x", 1:d))
))
lnr_kern_func <- generate_kernel(method = "linear")
kern_effect_mat <- 
  parse_kernel_variable("k(x3, x4)", lnr_kern_func, data)
beta_true <- c(1, .41, 2.37)
alpha_true <- rnorm(n)

K <- kern_effect_mat
data$y <- as.matrix(cbind(1, data[, c("x1", "x2")])) \%*\% beta_true + 
  K \%*\% alpha_true

model_matrices <- parse_cvek_formula(formula, 
                                     kern_func_list = kern_func_list, 
                                     data = data, verbose = FALSE)
estimate_ridge(Y = model_matrices$y, X = model_matrices$X, 
K = model_matrices$K[[1]], lambda = exp(-5))

}
\references{
Andreas Buja, Trevor Hastie, and Robert Tibshirani. (1989) 
Linear Smoothers and Additive Models. Ann. Statist. Volume 17, Number 2, 453-510.
}
\author{
Wenying Deng
}
