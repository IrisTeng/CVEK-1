\nonstopmode{}
\documentclass[a4paper]{book}
\usepackage[times,inconsolata,hyper]{Rd}
\usepackage{makeidx}
\usepackage[utf8]{inputenc} % @SET ENCODING@
% \usepackage{graphicx} % @USE GRAPHICX@
\makeindex{}
\begin{document}
\chapter*{}
\begin{center}
{\textbf{\huge Package `CVEK'}}
\par\bigskip{\large \today}
\end{center}
\begin{description}
\raggedright{}
\inputencoding{utf8}
\item[Title]\AsIs{Cross-Validated Kernel Ensemble}
\item[Version]\AsIs{0.2-0}
\item[Date]\AsIs{2018-06-27}
\item[Description]\AsIs{Using a library of base kernels, it learns a proper
generating function from data by directly minimizing the ensemble
model’s error, and tests whether the data is generated by the RKHS
under the null hypothesis.}
\item[Depends]\AsIs{R (>= 3.0.1), mvtnorm, MASS, psych, limSolve}
\item[License]\AsIs{GPL-2}
\item[Encoding]\AsIs{UTF-8}
\item[LazyData]\AsIs{true}
\item[RoxygenNote]\AsIs{6.0.1}
\item[Suggests]\AsIs{knitr, rmarkdown, testthat}
\item[VignetteBuilder]\AsIs{knitr}
\item[NeedsCompilation]\AsIs{no}
\item[Author]\AsIs{Wenying Deng [aut, cre],
Jeremiah Zhe Liu [ctb]}
\item[Maintainer]\AsIs{Wenying Deng }\email{wdeng@hsph.harvard.edu}\AsIs{}
\end{description}
\Rdcontents{\R{} topics documented:}
\inputencoding{utf8}
\HeaderA{define\_model}{Defining the Model}{define.Rul.model}
%
\begin{Description}\relax
Give the complete formula and generate the expected kernel library.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
define_model(formula, data, kern_par = NULL, fixed_num = 1,
  label_names = NULL)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{formula}] (formula) A symbolic description of the model to be fitted.

\item[\code{data}] (dataframe, n*(p+q1+q2)) A dataframe to be fitted. See Details.

\item[\code{kern\_par}] (dataframe, K*3) A dataframe indicating the parameters of
base kernels to fit random effects. See Details.

\item[\code{fixed\_num}] (integer) A numeric number specifying the dimension of
fixed effects.

\item[\code{label\_names}] (list) A character string indicating all the interior
variables included in each group of random effect. See Details.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
It processes data based on formula and label\_names and creates a kernel
library according to the parameters given in kern\_par.

* label\_names: for two groups of random effects with sizes q1 and q2
respectively, label\_names contains two elements. The length of the first
element is q1, indicating the names of q1 interiors variables, and the
length of second one is q2, indicating the names of q2 interiors variables.

* data: for a data with n observations and p+q variables (with sub-groups of
sizes (q1, q2), q=q1+q2), the dimension of dataframe is n*(p+q). All entries
should be numeric and the column name of response is "Y", while the column
names of q variables are the ones from label\_names.

* kern\_par: for a library of K kernels, the dimension of this dataframe is
K*3. Each row represents a kernel. The first column is method, with entries
of character class. The second and the third are l and p respectively, both
with entries of numeric class.
\end{Details}
%
\begin{Value}
\begin{ldescription}
\item[\code{Y}] (vector of length n) Reponses of the dataframe.

\item[\code{X}] (dataframe, n*p) Fixed effects variables in the dataframe (could
contains several subfactors).

\item[\code{Z1}] (dataframe, n*q1) The first group of random effects variables in
the dataframe (could contains several subfactors).

\item[\code{Z2}] (dataframe, n*q2) The second group of random effects variables in
the dataframe (could contains several subfactors).

\item[\code{kern\_list}] (list of length K) A list of kernel functions given by
user.
\end{ldescription}
\end{Value}
%
\begin{Author}\relax
Wenying Deng
\end{Author}
%
\begin{SeeAlso}\relax
method: \code{\LinkA{generate\_kernel}{generate.Rul.kernel}}
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}



kern_par <- data.frame(method = c("rbf", "polynomial", "matern"), 
l = c(.5, 1, 1.5), d = 1:3)
kern_par$method <- as.character(kern_par$method)
define_model(formula = Y ~ X + Z1 + Z2, data = mydata, kern_par, 
fixed_num = 1, label_names = list(Z1 = c("z1", "z2"), Z2 = c("z3", "z4")))



\end{ExampleCode}
\end{Examples}
\inputencoding{utf8}
\HeaderA{ensemble}{Estimating Ensemble Kernel Matrices}{ensemble}
%
\begin{Description}\relax
Give a list of estimated kernel matrices and their weights.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
ensemble(n, kern_size, strategy, beta_exp, error_mat, A_hat)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{n}] (integer) A numeric number specifying the number of observations.

\item[\code{kern\_size}] (integer, equals to K) A numeric number specifying the
number of kernels in the kernel library.

\item[\code{strategy}] (character) A character string indicating which ensemble
strategy is to be used.

\item[\code{beta\_exp}] (numeric/character) A numeric value specifying the parameter
when strategy = "exp" \code{\LinkA{ensemble\_exp}{ensemble.Rul.exp}}.

\item[\code{error\_mat}] (matrix, n*K) A n\bsl{}*K matrix indicating errors.

\item[\code{A\_hat}] (list of length K) A list of projection matrices for each
kernel in the kernel library.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
There are three ensemble strategies available here:

\bold{Empirical Risk Minimization (Stacking)}

After obtaining the estimated errors \eqn{\{\hat{\epsilon}_d\}_{d=1}^D}{}, we
estimate the ensemble weights \eqn{u=\{u_d\}_{d=1}^D}{} such that it minimizes
the overall error \deqn{\hat{u}=\underset{u \in \Delta}{argmin}\parallel
\sum_{d=1}^Du_d\hat{\epsilon}_d\parallel^2 \quad where\; \Delta=\{u | u \geq
0, \parallel u \parallel_1=1\}}{} Then produce the final ensemble prediction:
\deqn{\hat{h}=\sum_{d=1}^D \hat{u}_d h_d=\sum_{d=1}^D \hat{u}_d
A_{d,\hat{\lambda}_d}y=\hat{A}y}{} where \eqn{\hat{A}=\sum_{d=1}^D \hat{u}_d
A_{d,\hat{\lambda}_d}}{} is the ensemble matrix.

\bold{Simple Averaging}

Motivated by existing literature in omnibus kernel, we propose another way
to obtain the ensemble matrix by simply choosing unsupervised weights
\eqn{u_d=1/D}{} for \eqn{d=1,2,...D}{}.

\bold{Exponential Weighting}

Additionally, another scholar gives a new strategy to calculate weights
based on the estimated errors \eqn{\{\hat{\epsilon}_d\}_{d=1}^D}{}.
\deqn{u_d(\beta)=\frac{exp(-\parallel \hat{\epsilon}_d
\parallel_2^2/\beta)}{\sum_{d=1}^Dexp(-\parallel \hat{\epsilon}_d
\parallel_2^2/\beta)}}{}
\end{Details}
%
\begin{Value}
\begin{ldescription}
\item[\code{A\_est}] (matrix, n*n) A list of estimated kernel matrices.

\item[\code{u\_hat}] (vector of length K) A vector of weights of the kernels in the
library.
\end{ldescription}
\end{Value}
%
\begin{Author}\relax
Wenying Deng
\end{Author}
%
\begin{References}\relax
Jeremiah Zhe Liu and Brent Coull. Robust Hypothesis Test for
Nonlinear Effect with Gaussian Processes. October 2017.

Xiang Zhan, Anna Plantinga, Ni Zhao, and Michael C. Wu. A fast small-sample
kernel inde- pendence test for microbiome community-level association
analysis. December 2017.

Arnak S. Dalalyan and Alexandre B. Tsybakov. Aggregation by Exponential
Weighting and Sharp Oracle Inequalities. In Learning Theory, Lecture Notes
in Computer Science, pages 97– 111. Springer, Berlin, Heidelberg, June 2007.
\end{References}
%
\begin{SeeAlso}\relax
mode: \code{\LinkA{tuning}{tuning}}
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}



ensemble(n = 100, kern_size = 3, strategy = "stack", beta_exp = 1, 
CVEK:::error_mat, CVEK:::A_hat)



\end{ExampleCode}
\end{Examples}
\inputencoding{utf8}
\HeaderA{ensemble\_avg}{Estimating Ensemble Kernel Matrices Using AVG}{ensemble.Rul.avg}
%
\begin{Description}\relax
Give a list of estimated kernel matrices and their weights using simple
averaging.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
ensemble_avg(n, kern_size, beta_exp, error_mat, A_hat)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{n}] (integer) A numeric number specifying the number of observations.

\item[\code{kern\_size}] (integer, equals to K) A numeric number specifying the
number of kernels in the kernel library.

\item[\code{beta\_exp}] (numeric/character) A numeric value specifying the parameter
when strategy = "exp" \code{\LinkA{ensemble\_exp}{ensemble.Rul.exp}}.

\item[\code{error\_mat}] (matrix, n*K) A n\bsl{}*K matrix indicating errors.

\item[\code{A\_hat}] (list of length K) A list of projection matrices for each
kernel in the kernel library.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
\bold{Simple Averaging}

Motivated by existing literature in omnibus kernel, we propose another way
to obtain the ensemble matrix by simply choosing unsupervised weights
\eqn{u_d=1/D}{} for \eqn{d=1,2,...D}{}.
\end{Details}
%
\begin{Value}
\begin{ldescription}
\item[\code{A\_est}] (matrix, n*n) A list of estimated kernel matrices.

\item[\code{u\_hat}] (vector of length K) A vector of weights of the kernels in the
library.
\end{ldescription}
\end{Value}
%
\begin{Author}\relax
Wenying Deng
\end{Author}
%
\begin{References}\relax
Jeremiah Zhe Liu and Brent Coull. Robust Hypothesis Test for
Nonlinear Effect with Gaussian Processes. October 2017.

Xiang Zhan, Anna Plantinga, Ni Zhao, and Michael C. Wu. A fast small-sample
kernel inde- pendence test for microbiome community-level association
analysis. December 2017.

Arnak S. Dalalyan and Alexandre B. Tsybakov. Aggregation by Exponential
Weighting and Sharp Oracle Inequalities. In Learning Theory, Lecture Notes
in Computer Science, pages 97– 111. Springer, Berlin, Heidelberg, June 2007.
\end{References}
%
\begin{SeeAlso}\relax
mode: \code{\LinkA{tuning}{tuning}}
\end{SeeAlso}
\inputencoding{utf8}
\HeaderA{ensemble\_exp}{Estimating Ensemble Kernel Matrices Using EXP}{ensemble.Rul.exp}
%
\begin{Description}\relax
Give a list of estimated kernel matrices and their weights using exponential
weighting.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
ensemble_exp(n, kern_size, beta_exp, error_mat, A_hat)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{n}] (integer) A numeric number specifying the number of observations.

\item[\code{kern\_size}] (integer, equals to K) A numeric number specifying the
number of kernels in the kernel library.

\item[\code{beta\_exp}] (numeric/character) A numeric value specifying the parameter
when strategy = "exp". See Details.

\item[\code{error\_mat}] (matrix, n*K) A n\bsl{}*K matrix indicating errors.

\item[\code{A\_hat}] (list of length K) A list of projection matrices for each
kernel in the kernel library.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
\bold{Exponential Weighting}

Additionally, another scholar gives a new strategy to calculate weights
based on the estimated errors \eqn{\{\hat{\epsilon}_d\}_{d=1}^D}{}.
\deqn{u_d(\beta)=\frac{exp(-\parallel \hat{\epsilon}_d
\parallel_2^2/\beta)}{\sum_{d=1}^Dexp(-\parallel \hat{\epsilon}_d
\parallel_2^2/\beta)}}{}

\bold{beta\_exp}

The value of beta\_exp can be "min"=\eqn{min\{RSS\}_{d=1}^D/10}{},
"med"=\eqn{median\{RSS\}_{d=1}^D}{}, "max"=\eqn{max\{RSS\}_{d=1}^D*2}{} and any
other positive numeric number, where \eqn{\{RSS\} _{d=1}^D}{} are the set of
residual sum of squares of \eqn{D}{} base kernels.
\end{Details}
%
\begin{Value}
\begin{ldescription}
\item[\code{A\_est}] (matrix, n*n) A list of estimated kernel matrices.

\item[\code{u\_hat}] (vector of length K) A vector of weights of the kernels in the
library.
\end{ldescription}
\end{Value}
%
\begin{Author}\relax
Wenying Deng
\end{Author}
%
\begin{References}\relax
Jeremiah Zhe Liu and Brent Coull. Robust Hypothesis Test for
Nonlinear Effect with Gaussian Processes. October 2017.

Xiang Zhan, Anna Plantinga, Ni Zhao, and Michael C. Wu. A fast small-sample
kernel inde- pendence test for microbiome community-level association
analysis. December 2017.

Arnak S. Dalalyan and Alexandre B. Tsybakov. Aggregation by Exponential
Weighting and Sharp Oracle Inequalities. In Learning Theory, Lecture Notes
in Computer Science, pages 97– 111. Springer, Berlin, Heidelberg, June 2007.
\end{References}
%
\begin{SeeAlso}\relax
mode: \code{\LinkA{tuning}{tuning}}
\end{SeeAlso}
\inputencoding{utf8}
\HeaderA{ensemble\_stack}{Estimating Ensemble Kernel Matrices Using Stack}{ensemble.Rul.stack}
%
\begin{Description}\relax
Give a list of estimated kernel matrices and their weights using stacking.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
ensemble_stack(n, kern_size, beta_exp, error_mat, A_hat)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{n}] (integer) A numeric number specifying the number of observations.

\item[\code{kern\_size}] (integer, equals to K) A numeric number specifying the
number of kernels in the kernel library.

\item[\code{beta\_exp}] (numeric/character) A numeric value specifying the parameter
when strategy = "exp" \code{\LinkA{ensemble\_exp}{ensemble.Rul.exp}}.

\item[\code{error\_mat}] (matrix, n*K) A n\bsl{}*K matrix indicating errors.

\item[\code{A\_hat}] (list of length K) A list of projection matrices for each
kernel in the kernel library.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
\bold{Empirical Risk Minimization (Stacking)}

After obtaining the estimated errors \eqn{\{\hat{\epsilon}_d\}_{d=1}^D}{}, we
estimate the ensemble weights \eqn{u=\{u_d\}_{d=1}^D}{} such that it minimizes
the overall error \deqn{\hat{u}=\underset{u \in \Delta}{argmin}\parallel
\sum_{d=1}^Du_d\hat{\epsilon}_d\parallel^2 \quad where\; \Delta=\{u | u \geq
0, \parallel u \parallel_1=1\}}{} Then produce the final ensemble prediction:
\deqn{\hat{h}=\sum_{d=1}^D \hat{u}_d h_d=\sum_{d=1}^D \hat{u}_d
A_{d,\hat{\lambda}_d}y=\hat{A}y}{} where \eqn{\hat{A}=\sum_{d=1}^D \hat{u}_d
A_{d,\hat{\lambda}_d}}{} is the ensemble matrix.
\end{Details}
%
\begin{Value}
\begin{ldescription}
\item[\code{A\_est}] (matrix, n*n) A list of estimated kernel matrices.

\item[\code{u\_hat}] (vector of length K) A vector of weights of the kernels in the
library.
\end{ldescription}
\end{Value}
%
\begin{Author}\relax
Wenying Deng
\end{Author}
%
\begin{References}\relax
Jeremiah Zhe Liu and Brent Coull. Robust Hypothesis Test for
Nonlinear Effect with Gaussian Processes. October 2017.

Xiang Zhan, Anna Plantinga, Ni Zhao, and Michael C. Wu. A fast small-sample
kernel inde- pendence test for microbiome community-level association
analysis. December 2017.

Arnak S. Dalalyan and Alexandre B. Tsybakov. Aggregation by Exponential
Weighting and Sharp Oracle Inequalities. In Learning Theory, Lecture Notes
in Computer Science, pages 97– 111. Springer, Berlin, Heidelberg, June 2007.
\end{References}
%
\begin{SeeAlso}\relax
mode: \code{\LinkA{tuning}{tuning}}
\end{SeeAlso}
\inputencoding{utf8}
\HeaderA{estimate\_ridge}{Estimating a Single Model}{estimate.Rul.ridge}
%
\begin{Description}\relax
Estimating projection matrices and parameter estimates for a single model.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
estimate_ridge(X, K, Y, lambda)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{X}] (dataframe, n*p) Fixed effects variables in the dataframe (could
contains several subfactors).

\item[\code{K}] (matrix, n*n) Kernel matrix.

\item[\code{Y}] (vector of length n) Reponses of the dataframe.

\item[\code{lambda}] (numeric) A numeric string specifying the range of tuning parameter 
to be chosen. The lower limit of lambda must be above 0.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
For a single model, we can calculate the output of gaussian process
regression, the solution is given by \deqn{\hat{\beta}=[X^T(K+\lambda
I)^{-1}X]^{-1}X^T(K+\lambda I)^{-1}y}{} \deqn{\hat{\alpha}=(K+\lambda
I)^{-1}(y-\hat{\beta}X)}{}.
\end{Details}
%
\begin{Value}
\begin{ldescription}
\item[\code{lambda}] (numeric) The selected tuning parameter based on the
estimated ensemble kernel matrix.

\item[\code{beta}] (matrix, p*1) Fixed effects estimator of the model.

\item[\code{alpha}] (vector of length n) Random effects estimator of the estimated
ensemble kernel matrix.

\item[\code{proj\_matrix}] (list of length 4) Estimated projection matrices of the
model.
\end{ldescription}
\end{Value}
%
\begin{Author}\relax
Wenying Deng
\end{Author}
%
\begin{Examples}
\begin{ExampleCode}



estimate_ridge(X = cbind(matrix(1, nrow = 100, ncol = 1), CVEK:::X), 
CVEK:::K_ens, CVEK:::Y, lambda = exp(seq(-10, 5)))



\end{ExampleCode}
\end{Examples}
\inputencoding{utf8}
\HeaderA{estimation}{Conducting Gaussian Process Regression}{estimation}
%
\begin{Description}\relax
Conduct gaussian process regression based on the estimated ensemble kernel
matrix.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
estimation(Y, X, Z1, Z2, kern_list, mode = "loocv", strategy = "stack",
  beta_exp = 1, lambda = exp(seq(-10, 5)))
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{Y}] (vector of length n) Reponses of the dataframe.

\item[\code{X}] (dataframe, n*p) Fixed effects variables in the dataframe (could
contains several subfactors).

\item[\code{Z1}] (dataframe, n*q1) The first group of random effects variables in
the dataframe (could contains several subfactors).

\item[\code{Z2}] (dataframe, n*q2) The second group of random effects variables in
the dataframe (could contains several subfactors).

\item[\code{kern\_list}] (list of length K) A list of kernel functions given by
user.

\item[\code{mode}] (character) A character string indicating which tuning parameter
criteria is to be used.

\item[\code{strategy}] (character) A character string indicating which ensemble
strategy is to be used.

\item[\code{beta\_exp}] (numeric/character) A numeric value specifying the parameter
when strategy = "exp" \code{\LinkA{ensemble\_exp}{ensemble.Rul.exp}}.

\item[\code{lambda}] (numeric) A numeric string specifying the range of noise to be
chosen. The lower limit of lambda must be above 0.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
After obtaining the ensemble kernel matrix, we can calculate the outpur of
gaussian process regression.
\end{Details}
%
\begin{Value}
\begin{ldescription}
\item[\code{lambda}] (numeric) The selected tuning parameter based on the
estimated ensemble kernel matrix.

\item[\code{beta}] (matrix, p*1) Fixed effects estimator of the model.

\item[\code{alpha}] (vector of length n) Random effects estimator of the estimated
ensemble kernel matrix.

\item[\code{K}] (matrix, n*n) Estimated ensemble kernel matrix.

\item[\code{u\_hat}] (vector of length K) A vector of weights of the kernels in the
library.

\item[\code{base\_est}] (list of length 6) The detailed estimation results of K
kernels.
\end{ldescription}
\end{Value}
%
\begin{Author}\relax
Wenying Deng
\end{Author}
%
\begin{SeeAlso}\relax
strategy: \code{\LinkA{ensemble}{ensemble}}
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}



estimation(CVEK:::Y, CVEK:::X, CVEK:::Z1, CVEK:::Z2, CVEK:::kern_list, 
mode = "loocv", strategy = "stack", beta_exp = 1, lambda = exp(seq(-10, 5)))



\end{ExampleCode}
\end{Examples}
\inputencoding{utf8}
\HeaderA{generate\_data}{Generating Original Data}{generate.Rul.data}
%
\begin{Description}\relax
Generate original data based on specific kernels.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
generate_data(n, fixed_num = 1, label_names = NULL, method = "rbf",
  l = 1, d = 2, int_effect = 0, eps = 0.01)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{n}] (integer) A numeric number specifying the number of observations.

\item[\code{fixed\_num}] (integer) A numeric number specifying the dimension of
fixed effects.

\item[\code{label\_names}] (list) A character string indicating all the interior
variables included in each group of random effects.

\item[\code{method}] (character) A character string indicating which kernel is to
be computed.

\item[\code{l}] (numeric) A numeric number indicating the hyperparameter
(flexibility) of a specific kernel.

\item[\code{d}] (integer) For polynomial, d is the power; for matern, v = d + 1 /
2; for rational, alpha = d.

\item[\code{int\_effect}] (numeric) A numeric number specifying the size of
interaction.

\item[\code{eps}] (numeric) A numeric number indicating the size of noise of fixed
effects.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
This function generates with a specific dataset. The argument int\_effect
represents the strength of interaction of random effects relative to the
main random effects since all sampled functions have been standardized to
have unit norm.
\end{Details}
%
\begin{Value}
\begin{ldescription}
\item[\code{data}] (dataframe, n*(p+q)) A dataframe to be fitted.
\end{ldescription}
\end{Value}
%
\begin{Author}\relax
Wenying Deng
\end{Author}
%
\begin{Examples}
\begin{ExampleCode}



mydata <- generate_data(n = 100, fixed_num = 1, label_names =
list(Z1 = c("z1", "z2"), Z2 = c("z3", "z4")),
method = "rbf", l = 1, d = 2, int_effect = 0, eps = .01)



\end{ExampleCode}
\end{Examples}
\inputencoding{utf8}
\HeaderA{generate\_formula}{From Vectors to Single Variables}{generate.Rul.formula}
%
\begin{Description}\relax
Transform format of predictors from vectors to single variables.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
generate_formula(formula, label_names)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{formula}] (formula) A symbolic description of the model to be fitted.

\item[\code{label\_names}] (list) A character string indicating all the interior
variables included in each group of random effect.
\end{ldescription}
\end{Arguments}
%
\begin{Value}
\begin{ldescription}
\item[\code{generic\_formula}] (formula) A symbolic description of the model
written in single variables format.

\item[\code{length\_main}] (integer) A numeric value indicating the length of main
random effects.
\end{ldescription}
\end{Value}
%
\begin{Author}\relax
Wenying Deng
\end{Author}
%
\begin{Examples}
\begin{ExampleCode}



generic_formula0 <- generate_formula(formula = Y ~ X + Z1 + Z2,
label_names = list(Z1 = c("z1", "z2"), Z2 = c("z3", "z4")))



\end{ExampleCode}
\end{Examples}
\inputencoding{utf8}
\HeaderA{generate\_kernel}{Generating A Single Kernel}{generate.Rul.kernel}
%
\begin{Description}\relax
Generate kernels for the kernel library.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
generate_kernel(method = "rbf", l = 1, d = 2)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{method}] (character) A character string indicating which kernel 
is to be computed.

\item[\code{l}] (numeric) A numeric number indicating the hyperparameter 
(flexibility) of a specific kernel.

\item[\code{d}] (integer) For polynomial, d is the power; for matern, v = d + 1 / 2; for
rational, alpha = d.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
There are seven kinds of kernel available here. For convenience, we define
\eqn{r=\mid x-x'\mid}{}.

\bold{Gaussian RBF Kernels} \deqn{k_{SE}(r)=exp\Big(-\frac{r^2}{2l^2}\Big)}{}

\bold{Matern Kernels}
\deqn{k_{Matern}(r)=\frac{2^{1-\nu}}{\Gamma(\nu)}\Big(\frac{\sqrt{2\nu
r}}{l}\Big)^\nu K_\nu \Big(\frac{\sqrt{2\nu r}}{l}\Big)}{}

\bold{Rational Quadratic Kernels} \deqn{k_{RQ}(r)=\Big(1+\frac{r^2}{2\alpha
l^2}\Big)^{-\alpha}}{}

\bold{Polynomial Kernels} \deqn{k(x, x')=(x \cdot x')^d}{} We have intercept
kernel when \eqn{d=0}{}, and linear kernel when \eqn{d=1}{}.

\bold{Neural Network Kernels} \deqn{k_{NN}(x,
x')=\frac{2}{\pi}sin^{-1}\Big(\frac{2\tilde{x}^T
\tilde{x}'}{\sqrt{(1+2\tilde{x}^T \tilde{x})(1+2\tilde{x}'^T
\tilde{x}')}}\Big)}{}
\end{Details}
%
\begin{Value}
\begin{ldescription}
\item[\code{kern}] (function) A function indicating the generated kernel.
\end{ldescription}
\end{Value}
%
\begin{Author}\relax
Wenying Deng
\end{Author}
%
\begin{References}\relax
The MIT Press. Gaussian Processes for Machine Learning, 2006.
\end{References}
%
\begin{Examples}
\begin{ExampleCode}


kern_list <- list()
for (k in 1:nrow(kern_par)) {
  kern_list[[k]] <- generate_kernel(kern_par[k, ]$method,
                                    kern_par[k, ]$l,
                                    kern_par[k, ]$d)
}


\end{ExampleCode}
\end{Examples}
\inputencoding{utf8}
\HeaderA{kernel\_intercept}{Generating A Single Matrix-wise Function Using Intercept}{kernel.Rul.intercept}
%
\begin{Description}\relax
Generate matrix-wise functions for two matrices using intercept kernel.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
kernel_intercept(l, d)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{l}] (numeric) A numeric number indicating the hyperparameter
(flexibility) of a specific kernel.

\item[\code{d}] (integer) For polynomial, d is the power; for matern, v = d + 1 /
2; for rational, alpha = d.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
\bold{Polynomial Kernels} \deqn{k(x, x')=(x \cdot x')^d}{} We have intercept
kernel when \eqn{d=0}{}, and linear kernel when \eqn{d=1}{}.
\end{Details}
%
\begin{Value}
\begin{ldescription}
\item[\code{matrix\_wise}] (function) A function calculating the relevance
of two matrices.
\end{ldescription}
\end{Value}
%
\begin{Author}\relax
Wenying Deng
\end{Author}
%
\begin{References}\relax
The MIT Press. Gaussian Processes for Machine Learning, 2006.
\end{References}
\inputencoding{utf8}
\HeaderA{kernel\_linear}{Generating A Single Matrix-wise Function Using Linear}{kernel.Rul.linear}
%
\begin{Description}\relax
Generate matrix-wise functions for two matrices using linear kernel.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
kernel_linear(l, d)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{l}] (numeric) A numeric number indicating the hyperparameter
(flexibility) of a specific kernel.

\item[\code{d}] (integer) For polynomial, d is the power; for matern, v = d + 1 /
2; for rational, alpha = d.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
\bold{Polynomial Kernels} \deqn{k(x, x')=(x \cdot x')^d}{} We have intercept
kernel when \eqn{d=0}{}, and linear kernel when \eqn{d=1}{}.
\end{Details}
%
\begin{Value}
\begin{ldescription}
\item[\code{matrix\_wise}] (function) A function calculating the relevance
of two matrices.
\end{ldescription}
\end{Value}
%
\begin{Author}\relax
Wenying Deng
\end{Author}
%
\begin{References}\relax
The MIT Press. Gaussian Processes for Machine Learning, 2006.
\end{References}
\inputencoding{utf8}
\HeaderA{kernel\_matern}{Generating A Single Matrix-wise Function Using Matern}{kernel.Rul.matern}
%
\begin{Description}\relax
Generate matrix-wise functions for two matrices using matern kernel.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
kernel_matern(l, d)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{l}] (numeric) A numeric number indicating the hyperparameter
(flexibility) of a specific kernel.

\item[\code{d}] (integer) For polynomial, d is the power; for matern, v = d + 1 /
2; for rational, alpha = d.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
\bold{Matern Kernels}
\deqn{k_{Matern}(r)=\frac{2^{1-\nu}}{\Gamma(\nu)}\Big(\frac{\sqrt{2\nu
r}}{l}\Big)^\nu K_\nu \Big(\frac{\sqrt{2\nu r}}{l}\Big)}{}
\end{Details}
%
\begin{Value}
\begin{ldescription}
\item[\code{matrix\_wise}] (function) A function calculating the relevance
of two matrices.
\end{ldescription}
\end{Value}
%
\begin{Author}\relax
Wenying Deng
\end{Author}
%
\begin{References}\relax
The MIT Press. Gaussian Processes for Machine Learning, 2006.
\end{References}
\inputencoding{utf8}
\HeaderA{kernel\_nn}{Generating A Single Matrix-wise Function Using Neural Network}{kernel.Rul.nn}
%
\begin{Description}\relax
Generate matrix-wise functions for two matrices using neural network kernel.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
kernel_nn(l, d)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{l}] (numeric) A numeric number indicating the hyperparameter
(flexibility) of a specific kernel.

\item[\code{d}] (integer) For polynomial, d is the power; for matern, v = d + 1 /
2; for rational, alpha = d.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
\bold{Neural Network Kernels} \deqn{k_{NN}(x,
x')=\frac{2}{\pi}sin^{-1}\Big(\frac{2\tilde{x}^T
\tilde{x}'}{\sqrt{(1+2\tilde{x}^T \tilde{x})(1+2\tilde{x}'^T
\tilde{x}')}}\Big)}{}
\end{Details}
%
\begin{Value}
\begin{ldescription}
\item[\code{matrix\_wise}] (function) A function calculating the relevance
of two matrices.
\end{ldescription}
\end{Value}
%
\begin{Author}\relax
Wenying Deng
\end{Author}
%
\begin{References}\relax
The MIT Press. Gaussian Processes for Machine Learning, 2006.
\end{References}
\inputencoding{utf8}
\HeaderA{kernel\_polynomial}{Generating A Single Matrix-wise Function Using Polynomial}{kernel.Rul.polynomial}
%
\begin{Description}\relax
Generate matrix-wise functions for two matrices using polynomial kernel.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
kernel_polynomial(l, d)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{l}] (numeric) A numeric number indicating the hyperparameter
(flexibility) of a specific kernel.

\item[\code{d}] (integer) For polynomial, d is the power; for matern, v = d + 1 /
2; for rational, alpha = d.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
\bold{Polynomial Kernels} \deqn{k(x, x')=(x \cdot x')^d}{} We have intercept
kernel when \eqn{d=0}{}, and linear kernel when \eqn{d=1}{}.
\end{Details}
%
\begin{Value}
\begin{ldescription}
\item[\code{matrix\_wise}] (function) A function calculating the relevance
of two matrices.
\end{ldescription}
\end{Value}
%
\begin{Author}\relax
Wenying Deng
\end{Author}
%
\begin{References}\relax
The MIT Press. Gaussian Processes for Machine Learning, 2006.
\end{References}
\inputencoding{utf8}
\HeaderA{kernel\_rational}{Generating A Single Matrix-wise Function Using Rational Quadratic}{kernel.Rul.rational}
%
\begin{Description}\relax
Generate matrix-wise functions for two matrices using rational kernel.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
kernel_rational(l, d)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{l}] (numeric) A numeric number indicating the hyperparameter
(flexibility) of a specific kernel.

\item[\code{d}] (integer) For polynomial, d is the power; for matern, v = d + 1 /
2; for rational, alpha = d.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
\bold{Rational Quadratic Kernels} \deqn{k_{RQ}(r)=\Big(1+\frac{r^2}{2\alpha
l^2}\Big)^{-\alpha}}{}
\end{Details}
%
\begin{Value}
\begin{ldescription}
\item[\code{matrix\_wise}] (function) A function calculating the relevance
of two matrices.
\end{ldescription}
\end{Value}
%
\begin{Author}\relax
Wenying Deng
\end{Author}
%
\begin{References}\relax
The MIT Press. Gaussian Processes for Machine Learning, 2006.
\end{References}
\inputencoding{utf8}
\HeaderA{kernel\_rbf}{Generating A Single Matrix-wise Function Using RBF}{kernel.Rul.rbf}
%
\begin{Description}\relax
Generate matrix-wise functions for two matrices using rbf kernel.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
kernel_rbf(l, d)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{l}] (numeric) A numeric number indicating the hyperparameter
(flexibility) of a specific kernel.

\item[\code{d}] (integer) For polynomial, d is the power; for matern, v = d + 1 /
2; for rational, alpha = d.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
\bold{Gaussian RBF Kernels} \deqn{k_{SE}(r)=exp\Big(-\frac{r^2}{2l^2}\Big)}{}
\end{Details}
%
\begin{Value}
\begin{ldescription}
\item[\code{matrix\_wise}] (function) A function calculating the relevance
of two matrices.
\end{ldescription}
\end{Value}
%
\begin{Author}\relax
Wenying Deng
\end{Author}
%
\begin{References}\relax
The MIT Press. Gaussian Processes for Machine Learning, 2006.
\end{References}
\inputencoding{utf8}
\HeaderA{kern\_par}{Dataframe as an example for kernel parameters}{kern.Rul.par}
\keyword{datasets}{kern\_par}
%
\begin{Description}\relax
A dataframe containing the information from user to define base kernels.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
data(kern_par)
\end{verbatim}
\end{Usage}
%
\begin{Format}
A data frame with 3 rows and 3 variables
\end{Format}
%
\begin{Details}\relax
\begin{itemize}

\item method. (character) A character string indicating which kernel 
is to be computed.
\item l. (numeric) A numeric number indicating the hyperparameter 
(flexibility) of a specific kernel.
\item d. (integer) For polynomial, d is the power; for matern, v = d + 1 / 2; for
rational, alpha = d.

\end{itemize}

\end{Details}
\inputencoding{utf8}
\HeaderA{mydata}{Dataset as an example}{mydata}
\keyword{datasets}{mydata}
%
\begin{Description}\relax
A dataset containing one column of fixed effects variable, two groups of
random effects variables, each with two columns of variables respectively. 
The variables are as follows:
\end{Description}
%
\begin{Usage}
\begin{verbatim}
data(mydata)
\end{verbatim}
\end{Usage}
%
\begin{Format}
A data frame with 100 rows and 5 variables
\end{Format}
%
\begin{Details}\relax
\begin{itemize}

\item Y. (vector of length n) Reponses of the dataframe.
\item x1. (dataframe, n*1) Fixed effects variables in the dataframe.
\item z1. (dataframe, n*1) The first column of the first group of random effects 
variables in the dataframe.
\item z2. (dataframe, n*1) The second column of the first group of random effects 
variables in the dataframe.
\item z3. (dataframe, n*1) The first column of the second group of random effects 
variables in the dataframe.
\item z4. (dataframe, n*1) The second column of the second group of random effects 
variables in the dataframe.

\end{itemize}

\end{Details}
\inputencoding{utf8}
\HeaderA{testing}{Conducting Score Tests for Interaction}{testing}
%
\begin{Description}\relax
Conduct score tests comparing a fitted model and a more general alternative
model.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
testing(formula_int, label_names, Y, X, Z1, Z2, kern_list, mode = "loocv",
  strategy = "stack", beta_exp = 1, test = "boot", lambda = exp(seq(-10,
  5)), B = 100)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{formula\_int}] (formula) A symbolic description of the model with
interaction.

\item[\code{label\_names}] (list) A character string indicating all the interior
variables included in each group of random effect. See Details.

\item[\code{Y}] (vector of length n) Reponses of the dataframe.

\item[\code{X}] (dataframe, n*p) Fixed effects variables in the dataframe (could
contains several subfactors).

\item[\code{Z1}] (dataframe, n*q1) The first group of random effects variables in
the dataframe (could contains several subfactors).

\item[\code{Z2}] (dataframe, n*q2) The second group of random effects variables in
the dataframe (could contains several subfactors).

\item[\code{kern\_list}] (list of length K) A list of kernel functions given by
user.

\item[\code{mode}] (character) A character string indicating which tuning parameter
criteria is to be used.

\item[\code{strategy}] (character) A character string indicating which ensemble
strategy is to be used.

\item[\code{beta\_exp}] (numeric/character) A numeric value specifying the parameter
when strategy = "exp" \code{\LinkA{ensemble\_exp}{ensemble.Rul.exp}}.

\item[\code{test}] (character) A character string indicating which test is to be
used.

\item[\code{lambda}] (numeric) A numeric string specifying the range of tuning parameter 
to be chosen. The lower limit of lambda must be above 0.

\item[\code{B}] (integer) A numeric value indicating times of resampling when test
= "boot".
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
There are two tests available here:

\bold{Asymptotic Test}

This is based on the classical variance component test to construct a
testing procedure for the hypothesis about Gaussian process function.

\bold{Bootstrap Test}

When it comes to small sample size, we can use bootstrap test instead, which
can give valid tests with moderate sample sizes and requires similar
computational effort to a permutation test.
\end{Details}
%
\begin{Value}
\begin{ldescription}
\item[\code{pvalue}] (numeric) p-value of the test.
\item[\code{lambda}] (numeric) The selected tuning parameter based on the estimated
ensemble kernel matrix.\item[\code{u\_hat}] (vector of length K) A vector of
weights of the kernels in the library.
\end{ldescription}
\end{Value}
%
\begin{Author}\relax
Wenying Deng
\end{Author}
%
\begin{References}\relax
Xihong Lin. Variance component testing in generalised linear
models with random effects. June 1997.

Arnab Maity and Xihong Lin. Powerful tests for detecting a gene effect in
the presence of possible gene-gene interactions using garrote kernel
machines. December 2011.

Petra Bu ̊zˇkova ́, Thomas Lumley, and Kenneth Rice. Permutation and
parametric bootstrap tests for gene-gene and gene-environment interactions.
January 2011.
\end{References}
%
\begin{SeeAlso}\relax
method: \code{\LinkA{generate\_kernel}{generate.Rul.kernel}}

mode: \code{\LinkA{tuning}{tuning}}

strategy: \code{\LinkA{ensemble}{ensemble}}
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}



testing(formula_int = Y ~ X + Z1 * Z2,
label_names = list(Z1 = c("z1", "z2"), Z2 = c("z3", "z4")),
CVEK:::Y, CVEK:::X, CVEK:::Z1, CVEK:::Z2, CVEK:::kern_list, 
mode = "loocv", strategy = "stack",
beta_exp = 1, test = "boot", lambda = exp(seq(-10, 5)), B = 100)



\end{ExampleCode}
\end{Examples}
\inputencoding{utf8}
\HeaderA{test\_asym}{Conducting Score Tests for Interaction Using Asymptotic Test}{test.Rul.asym}
%
\begin{Description}\relax
Conduct score tests comparing a fitted model and a more general alternative
model using asymptotic test.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
test_asym(n, Y, X, Z12, y_fixed, alpha0, K_ens, sigma2_hat, tau_hat, B)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{n}] (integer) A numeric number specifying the number of observations.

\item[\code{Y}] (vector of length n) Reponses of the dataframe.

\item[\code{X}] (dataframe, n*p) Fixed effects variables in the dataframe (could
contains several subfactors).

\item[\code{Z12}] (dataframe, n*(q1\bsl{}*q2)) The interaction items of the first and
second groups of random effects variables in the dataframe.

\item[\code{y\_fixed}] (vector of length n) Estimated fixed effects of the
responses.

\item[\code{alpha0}] (vector of length n) Random effects estimator of the estimated
ensemble kernel matrix.

\item[\code{K\_ens}] (matrix, n*n) Estimated ensemble kernel matrix.

\item[\code{sigma2\_hat}] (numeric) The estimated noise of the fixed effects.

\item[\code{tau\_hat}] (numeric) The estimated noise of the random effects.

\item[\code{B}] (integer) A numeric value indicating times of resampling when test
= "boot".
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
\bold{Asymptotic Test}

This is based on the classical variance component test to construct a
testing procedure for the hypothesis about Gaussian process function.
\end{Details}
%
\begin{Value}
\begin{ldescription}
\item[\code{pvalue}] (numeric) p-value of the test.
\end{ldescription}
\end{Value}
%
\begin{Author}\relax
Wenying Deng
\end{Author}
%
\begin{References}\relax
Xihong Lin. Variance component testing in generalised linear
models with random effects. June 1997.

Arnab Maity and Xihong Lin. Powerful tests for detecting a gene effect in
the presence of possible gene-gene interactions using garrote kernel
machines. December 2011.

Petra Bu ̊zˇkova ́, Thomas Lumley, and Kenneth Rice. Permutation and
parametric bootstrap tests for gene-gene and gene-environment interactions.
January 2011.
\end{References}
%
\begin{SeeAlso}\relax
method: \code{\LinkA{generate\_kernel}{generate.Rul.kernel}}

mode: \code{\LinkA{tuning}{tuning}}

strategy: \code{\LinkA{ensemble}{ensemble}}
\end{SeeAlso}
\inputencoding{utf8}
\HeaderA{test\_boot}{Conducting Score Tests for Interaction Using Bootstrap Test}{test.Rul.boot}
%
\begin{Description}\relax
Conduct score tests comparing a fitted model and a more general alternative
model using bootstrap test.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
test_boot(n, Y, X, Z12, y_fixed, alpha0, K_ens, sigma2_hat, tau_hat, B)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{n}] (integer) A numeric number specifying the number of observations.

\item[\code{Y}] (vector of length n) Reponses of the dataframe.

\item[\code{X}] (dataframe, n*p) Fixed effects variables in the dataframe (could
contains several subfactors).

\item[\code{Z12}] (dataframe, n*(q1\bsl{}*q2)) The interaction items of the first and
second groups of random effects variables in the dataframe.

\item[\code{y\_fixed}] (vector of length n) Estimated fixed effects of the
responses.

\item[\code{alpha0}] (vector of length n) Random effects estimator of the estimated
ensemble kernel matrix.

\item[\code{K\_ens}] (matrix, n*n) Estimated ensemble kernel matrix.

\item[\code{sigma2\_hat}] (numeric) The estimated noise of the fixed effects.

\item[\code{tau\_hat}] (numeric) The estimated noise of the random effects.

\item[\code{B}] (integer) A numeric value indicating times of resampling when test
= "boot".
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
\bold{Bootstrap Test}

When it comes to small sample size, we can use bootstrap test instead, which
can give valid tests with moderate sample sizes and requires similar
computational effort to a permutation test.
\end{Details}
%
\begin{Value}
\begin{ldescription}
\item[\code{pvalue}] (numeric) p-value of the test.
\end{ldescription}
\end{Value}
%
\begin{Author}\relax
Wenying Deng
\end{Author}
%
\begin{References}\relax
Xihong Lin. Variance component testing in generalised linear
models with random effects. June 1997.

Arnab Maity and Xihong Lin. Powerful tests for detecting a gene effect in
the presence of possible gene-gene interactions using garrote kernel
machines. December 2011.

Petra Bu ̊zˇkova ́, Thomas Lumley, and Kenneth Rice. Permutation and
parametric bootstrap tests for gene-gene and gene-environment interactions.
January 2011.
\end{References}
%
\begin{SeeAlso}\relax
method: \code{\LinkA{generate\_kernel}{generate.Rul.kernel}}

mode: \code{\LinkA{tuning}{tuning}}

strategy: \code{\LinkA{ensemble}{ensemble}}
\end{SeeAlso}
\inputencoding{utf8}
\HeaderA{tuning}{Calculating Tuning Parameters}{tuning}
%
\begin{Description}\relax
Calculate tuning parameters based on given criteria.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
tuning(Y, X, K_mat, mode, lambda)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{Y}] (vector of length n) Reponses of the dataframe.

\item[\code{X}] (dataframe, n*p) Fixed effects variables in the dataframe (could
contains several subfactors).

\item[\code{K\_mat}] (matrix, n*n) Estimated ensemble kernel matrix.

\item[\code{mode}] (character) A character string indicating which tuning parameter
criteria is to be used.

\item[\code{lambda}] (numeric) A numeric string specifying the range of tuning parameter 
to be chosen. The lower limit of lambda must be above 0.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
There are seven tuning parameter selections here:

\bold{leave-one-out Cross Validation}

\deqn{\lambda_{n-CV}=\underset{\lambda \in
\Lambda}{argmin}\;\Big\{log\;y^{\star
T}[I-diag(A_\lambda)-\frac{1}{n}I]^{-1}(I-A_\lambda)^2[I-diag(A_\lambda)-
\frac{1}{n}I]^{-1}y^\star \Big\}}{}

\bold{Akaike Information Criteria}

\deqn{\lambda_{AIC}=\underset{\lambda \in \Lambda}{argmin}\Big\{log\;
y^{\star T}(I-A_\lambda)^2y^\star+\frac{2[tr(A_\lambda)+2]}{n}\Big\}}{}

\bold{Akaike Information Criteria (small sample size)}

\deqn{\lambda_{AICc}=\underset{\lambda \in \Lambda}{argmin}\Big\{log\;
y^{\star
T}(I-A_\lambda)^2y^\star+\frac{2[tr(A_\lambda)+2]}{n-tr(A_\lambda)-3}\Big\}}{}

\bold{Bayesian Information Criteria}

\deqn{\lambda_{BIC}=\underset{\lambda \in \Lambda}{argmin}\Big\{log\;
y^{\star T}(I-A_\lambda)^2y^\star+\frac{log(n)[tr(A_\lambda)+2]}{n}\Big\}}{}

\bold{Generalized Cross Validation}

\deqn{\lambda_{GCV}=\underset{\lambda \in \Lambda}{argmin}\Big\{log\;
y^{\star
T}(I-A_\lambda)^2y^\star-2log[1-\frac{tr(A_\lambda)}{n}-\frac{1}{n}]_+\Big\}}{}

\bold{Generalized Cross Validation (small sample size)}

\deqn{\lambda_{GCVc}=\underset{\lambda \in \Lambda}{argmin}\Big\{log\;
y^{\star
T}(I-A_\lambda)^2y^\star-2log[1-\frac{tr(A_\lambda)}{n}-\frac{2}{n}]_+\Big\}}{}

\bold{Generalized Maximum Profile Marginal Likelihood}

\deqn{\lambda_{GMPML}=\underset{\lambda \in \Lambda}{argmin}\Big\{log\;
y^{\star T}(I-A_\lambda)y^\star-\frac{1}{n-1}log \mid I-A_\lambda \mid
\Big\}}{}
\end{Details}
%
\begin{Value}
\begin{ldescription}
\item[\code{lambda0}] (numeric) The estimated tuning parameter.
\end{ldescription}
\end{Value}
%
\begin{Author}\relax
Wenying Deng
\end{Author}
%
\begin{References}\relax
Philip S. Boonstra, Bhramar Mukherjee, and Jeremy M. G. Taylor.
A Small-Sample Choice of the Tuning Parameter in Ridge Regression. July
2015.

Trevor Hastie, Robert Tibshirani, and Jerome Friedman. The Elements of
Statistical Learning: Data Mining, Inference, and Prediction, Second
Edition. Springer Series in Statistics. Springer- Verlag, New York, 2
edition, 2009.

Hirotogu Akaike. Information Theory and an Extension of the Maximum
Likelihood Princi- ple. In Selected Papers of Hirotugu Akaike, Springer
Series in Statistics, pages 199–213. Springer, New York, NY, 1998.

Clifford M. Hurvich and Chih-Ling Tsai. Regression and time series model
selection in small samples. June 1989.

Hurvich Clifford M., Simonoff Jeffrey S., and Tsai Chih-Ling. Smoothing
parameter selection in nonparametric regression using an improved Akaike
information criterion. January 2002.
\end{References}
%
\begin{Examples}
\begin{ExampleCode}



lambda0 <- tuning(CVEK:::Y, CVEK:::X, K_mat = CVEK:::K_ens, 
mode = "loocv", lambda = exp(seq(-10, 5)))



\end{ExampleCode}
\end{Examples}
\inputencoding{utf8}
\HeaderA{tuning\_AIC}{Calculating Tuning Parameters Using AIC}{tuning.Rul.AIC}
%
\begin{Description}\relax
Calculate tuning parameters based on AIC.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
tuning_AIC(Y, X, K_mat, lambda)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{Y}] (vector of length n) Reponses of the dataframe.

\item[\code{X}] (dataframe, n*p) Fixed effects variables in the dataframe (could
contains several subfactors).

\item[\code{K\_mat}] (matrix, n*n) Estimated ensemble kernel matrix.

\item[\code{lambda}] (numeric) A numeric string specifying the range of tuning parameter 
to be chosen. The lower limit of lambda must be above 0.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
\bold{Akaike Information Criteria}

\deqn{\lambda_{AIC}=\underset{\lambda \in \Lambda}{argmin}\Big\{log\;
y^{\star T}(I-A_\lambda)^2y^\star+\frac{2[tr(A_\lambda)+2]}{n}\Big\}}{}
\end{Details}
%
\begin{Value}
\begin{ldescription}
\item[\code{lambda0}] (numeric) The estimated tuning parameter.
\end{ldescription}
\end{Value}
%
\begin{Author}\relax
Wenying Deng
\end{Author}
%
\begin{References}\relax
Philip S. Boonstra, Bhramar Mukherjee, and Jeremy M. G. Taylor.
A Small-Sample Choice of the Tuning Parameter in Ridge Regression. July
2015.

Trevor Hastie, Robert Tibshirani, and Jerome Friedman. The Elements of
Statistical Learning: Data Mining, Inference, and Prediction, Second
Edition. Springer Series in Statistics. Springer- Verlag, New York, 2
edition, 2009.

Hirotogu Akaike. Information Theory and an Extension of the Maximum
Likelihood Princi- ple. In Selected Papers of Hirotugu Akaike, Springer
Series in Statistics, pages 199–213. Springer, New York, NY, 1998.

Clifford M. Hurvich and Chih-Ling Tsai. Regression and time series model
selection in small samples. June 1989.

Hurvich Clifford M., Simonoff Jeffrey S., and Tsai Chih-Ling. Smoothing
parameter selection in nonparametric regression using an improved Akaike
information criterion. January 2002.
\end{References}
\inputencoding{utf8}
\HeaderA{tuning\_AICc}{Calculating Tuning Parameters Using AICc}{tuning.Rul.AICc}
%
\begin{Description}\relax
Calculate tuning parameters based on AICc.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
tuning_AICc(Y, X, K_mat, lambda)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{Y}] (vector of length n) Reponses of the dataframe.

\item[\code{X}] (dataframe, n*p) Fixed effects variables in the dataframe (could
contains several subfactors).

\item[\code{K\_mat}] (matrix, n*n) Estimated ensemble kernel matrix.

\item[\code{lambda}] (numeric) A numeric string specifying the range of tuning parameter 
to be chosen. The lower limit of lambda must be above 0.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
\bold{Akaike Information Criteria (small sample size)}

\deqn{\lambda_{AICc}=\underset{\lambda \in \Lambda}{argmin}\Big\{log\;
y^{\star
T}(I-A_\lambda)^2y^\star+\frac{2[tr(A_\lambda)+2]}{n-tr(A_\lambda)-3}\Big\}}{}
\end{Details}
%
\begin{Value}
\begin{ldescription}
\item[\code{lambda0}] (numeric) The estimated tuning parameter.
\end{ldescription}
\end{Value}
%
\begin{Author}\relax
Wenying Deng
\end{Author}
%
\begin{References}\relax
Philip S. Boonstra, Bhramar Mukherjee, and Jeremy M. G. Taylor.
A Small-Sample Choice of the Tuning Parameter in Ridge Regression. July
2015.

Trevor Hastie, Robert Tibshirani, and Jerome Friedman. The Elements of
Statistical Learning: Data Mining, Inference, and Prediction, Second
Edition. Springer Series in Statistics. Springer- Verlag, New York, 2
edition, 2009.

Hirotogu Akaike. Information Theory and an Extension of the Maximum
Likelihood Princi- ple. In Selected Papers of Hirotugu Akaike, Springer
Series in Statistics, pages 199–213. Springer, New York, NY, 1998.

Clifford M. Hurvich and Chih-Ling Tsai. Regression and time series model
selection in small samples. June 1989.

Hurvich Clifford M., Simonoff Jeffrey S., and Tsai Chih-Ling. Smoothing
parameter selection in nonparametric regression using an improved Akaike
information criterion. January 2002.
\end{References}
\inputencoding{utf8}
\HeaderA{tuning\_BIC}{Calculating Tuning Parameters Using BIC}{tuning.Rul.BIC}
%
\begin{Description}\relax
Calculate tuning parameters based on BIC.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
tuning_BIC(Y, X, K_mat, lambda)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{Y}] (vector of length n) Reponses of the dataframe.

\item[\code{X}] (dataframe, n*p) Fixed effects variables in the dataframe (could
contains several subfactors).

\item[\code{K\_mat}] (matrix, n*n) Estimated ensemble kernel matrix.

\item[\code{lambda}] (numeric) A numeric string specifying the range of tuning parameter 
to be chosen. The lower limit of lambda must be above 0.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
\bold{Bayesian Information Criteria}

\deqn{\lambda_{BIC}=\underset{\lambda \in \Lambda}{argmin}\Big\{log\;
y^{\star T}(I-A_\lambda)^2y^\star+\frac{log(n)[tr(A_\lambda)+2]}{n}\Big\}}{}
\end{Details}
%
\begin{Value}
\begin{ldescription}
\item[\code{lambda0}] (numeric) The estimated tuning parameter.
\end{ldescription}
\end{Value}
%
\begin{Author}\relax
Wenying Deng
\end{Author}
%
\begin{References}\relax
Philip S. Boonstra, Bhramar Mukherjee, and Jeremy M. G. Taylor.
A Small-Sample Choice of the Tuning Parameter in Ridge Regression. July
2015.

Trevor Hastie, Robert Tibshirani, and Jerome Friedman. The Elements of
Statistical Learning: Data Mining, Inference, and Prediction, Second
Edition. Springer Series in Statistics. Springer- Verlag, New York, 2
edition, 2009.

Hirotogu Akaike. Information Theory and an Extension of the Maximum
Likelihood Princi- ple. In Selected Papers of Hirotugu Akaike, Springer
Series in Statistics, pages 199–213. Springer, New York, NY, 1998.

Clifford M. Hurvich and Chih-Ling Tsai. Regression and time series model
selection in small samples. June 1989.

Hurvich Clifford M., Simonoff Jeffrey S., and Tsai Chih-Ling. Smoothing
parameter selection in nonparametric regression using an improved Akaike
information criterion. January 2002.
\end{References}
\inputencoding{utf8}
\HeaderA{tuning\_GCV}{Calculating Tuning Parameters Using GCV}{tuning.Rul.GCV}
%
\begin{Description}\relax
Calculate tuning parameters based on GCV.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
tuning_GCV(Y, X, K_mat, lambda)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{Y}] (vector of length n) Reponses of the dataframe.

\item[\code{X}] (dataframe, n*p) Fixed effects variables in the dataframe (could
contains several subfactors).

\item[\code{K\_mat}] (matrix, n*n) Estimated ensemble kernel matrix.

\item[\code{lambda}] (numeric) A numeric string specifying the range of tuning parameter 
to be chosen. The lower limit of lambda must be above 0.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
\bold{Generalized Cross Validation}

\deqn{\lambda_{GCV}=\underset{\lambda \in \Lambda}{argmin}\Big\{log\;
y^{\star
T}(I-A_\lambda)^2y^\star-2log[1-\frac{tr(A_\lambda)}{n}-\frac{1}{n}]_+\Big\}}{}
\end{Details}
%
\begin{Value}
\begin{ldescription}
\item[\code{lambda0}] (numeric) The estimated tuning parameter.
\end{ldescription}
\end{Value}
%
\begin{Author}\relax
Wenying Deng
\end{Author}
%
\begin{References}\relax
Philip S. Boonstra, Bhramar Mukherjee, and Jeremy M. G. Taylor.
A Small-Sample Choice of the Tuning Parameter in Ridge Regression. July
2015.

Trevor Hastie, Robert Tibshirani, and Jerome Friedman. The Elements of
Statistical Learning: Data Mining, Inference, and Prediction, Second
Edition. Springer Series in Statistics. Springer- Verlag, New York, 2
edition, 2009.

Hirotogu Akaike. Information Theory and an Extension of the Maximum
Likelihood Princi- ple. In Selected Papers of Hirotugu Akaike, Springer
Series in Statistics, pages 199–213. Springer, New York, NY, 1998.

Clifford M. Hurvich and Chih-Ling Tsai. Regression and time series model
selection in small samples. June 1989.

Hurvich Clifford M., Simonoff Jeffrey S., and Tsai Chih-Ling. Smoothing
parameter selection in nonparametric regression using an improved Akaike
information criterion. January 2002.
\end{References}
\inputencoding{utf8}
\HeaderA{tuning\_GCVc}{Calculating Tuning Parameters Using GCVc}{tuning.Rul.GCVc}
%
\begin{Description}\relax
Calculate tuning parameters based on GCVc.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
tuning_GCVc(Y, X, K_mat, lambda)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{Y}] (vector of length n) Reponses of the dataframe.

\item[\code{X}] (dataframe, n*p) Fixed effects variables in the dataframe (could
contains several subfactors).

\item[\code{K\_mat}] (matrix, n*n) Estimated ensemble kernel matrix.

\item[\code{lambda}] (numeric) A numeric string specifying the range of tuning parameter 
to be chosen. The lower limit of lambda must be above 0.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
\bold{Generalized Cross Validation (small sample size)}

\deqn{\lambda_{GCVc}=\underset{\lambda \in \Lambda}{argmin}\Big\{log\;
y^{\star
T}(I-A_\lambda)^2y^\star-2log[1-\frac{tr(A_\lambda)}{n}-\frac{2}{n}]_+\Big\}}{}
\end{Details}
%
\begin{Value}
\begin{ldescription}
\item[\code{lambda0}] (numeric) The estimated tuning parameter.
\end{ldescription}
\end{Value}
%
\begin{Author}\relax
Wenying Deng
\end{Author}
%
\begin{References}\relax
Philip S. Boonstra, Bhramar Mukherjee, and Jeremy M. G. Taylor.
A Small-Sample Choice of the Tuning Parameter in Ridge Regression. July
2015.

Trevor Hastie, Robert Tibshirani, and Jerome Friedman. The Elements of
Statistical Learning: Data Mining, Inference, and Prediction, Second
Edition. Springer Series in Statistics. Springer- Verlag, New York, 2
edition, 2009.

Hirotogu Akaike. Information Theory and an Extension of the Maximum
Likelihood Princi- ple. In Selected Papers of Hirotugu Akaike, Springer
Series in Statistics, pages 199–213. Springer, New York, NY, 1998.

Clifford M. Hurvich and Chih-Ling Tsai. Regression and time series model
selection in small samples. June 1989.

Hurvich Clifford M., Simonoff Jeffrey S., and Tsai Chih-Ling. Smoothing
parameter selection in nonparametric regression using an improved Akaike
information criterion. January 2002.
\end{References}
\inputencoding{utf8}
\HeaderA{tuning\_gmpml}{Calculating Tuning Parameters Using GMPML}{tuning.Rul.gmpml}
%
\begin{Description}\relax
Calculate tuning parameters based on Generalized Maximum Profile Marginal
Likelihood.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
tuning_gmpml(Y, X, K_mat, lambda)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{Y}] (vector of length n) Reponses of the dataframe.

\item[\code{X}] (dataframe, n*p) Fixed effects variables in the dataframe (could
contains several subfactors).

\item[\code{K\_mat}] (matrix, n*n) Estimated ensemble kernel matrix.

\item[\code{lambda}] (numeric) A numeric string specifying the range of tuning parameter 
to be chosen. The lower limit of lambda must be above 0.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
\bold{Generalized Maximum Profile Marginal Likelihood}

\deqn{\lambda_{GMPML}=\underset{\lambda \in \Lambda}{argmin}\Big\{log\;
y^{\star T}(I-A_\lambda)y^\star-\frac{1}{n-1}log \mid I-A_\lambda \mid
\Big\}}{}
\end{Details}
%
\begin{Value}
\begin{ldescription}
\item[\code{lambda0}] (numeric) The estimated tuning parameter.
\end{ldescription}
\end{Value}
%
\begin{Author}\relax
Wenying Deng
\end{Author}
%
\begin{References}\relax
Philip S. Boonstra, Bhramar Mukherjee, and Jeremy M. G. Taylor.
A Small-Sample Choice of the Tuning Parameter in Ridge Regression. July
2015.

Trevor Hastie, Robert Tibshirani, and Jerome Friedman. The Elements of
Statistical Learning: Data Mining, Inference, and Prediction, Second
Edition. Springer Series in Statistics. Springer- Verlag, New York, 2
edition, 2009.

Hirotogu Akaike. Information Theory and an Extension of the Maximum
Likelihood Princi- ple. In Selected Papers of Hirotugu Akaike, Springer
Series in Statistics, pages 199–213. Springer, New York, NY, 1998.

Clifford M. Hurvich and Chih-Ling Tsai. Regression and time series model
selection in small samples. June 1989.

Hurvich Clifford M., Simonoff Jeffrey S., and Tsai Chih-Ling. Smoothing
parameter selection in nonparametric regression using an improved Akaike
information criterion. January 2002.
\end{References}
\inputencoding{utf8}
\HeaderA{tuning\_loocv}{Calculating Tuning Parameters Using looCV}{tuning.Rul.loocv}
%
\begin{Description}\relax
Calculate tuning parameters based on given leave-one-out Cross Validation.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
tuning_loocv(Y, X, K_mat, lambda)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{Y}] (vector of length n) Reponses of the dataframe.

\item[\code{X}] (dataframe, n*p) Fixed effects variables in the dataframe (could
contains several subfactors).

\item[\code{K\_mat}] (matrix, n*n) Estimated ensemble kernel matrix.

\item[\code{lambda}] (numeric) A numeric string specifying the range of tuning parameter 
to be chosen. The lower limit of lambda must be above 0.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
\bold{leave-one-out Cross Validation}

\deqn{\lambda_{n-CV}=\underset{\lambda \in
\Lambda}{argmin}\;\Big\{log\;y^{\star
T}[I-diag(A_\lambda)-\frac{1}{n}I]^{-1}(I-A_\lambda)^2[I-diag(A_\lambda)-
\frac{1}{n}I]^{-1}y^\star \Big\}}{}
\end{Details}
%
\begin{Value}
\begin{ldescription}
\item[\code{lambda0}] (numeric) The estimated tuning parameter.
\end{ldescription}
\end{Value}
%
\begin{Author}\relax
Wenying Deng
\end{Author}
%
\begin{References}\relax
Philip S. Boonstra, Bhramar Mukherjee, and Jeremy M. G. Taylor.
A Small-Sample Choice of the Tuning Parameter in Ridge Regression. July
2015.

Trevor Hastie, Robert Tibshirani, and Jerome Friedman. The Elements of
Statistical Learning: Data Mining, Inference, and Prediction, Second
Edition. Springer Series in Statistics. Springer- Verlag, New York, 2
edition, 2009.

Hirotogu Akaike. Information Theory and an Extension of the Maximum
Likelihood Princi- ple. In Selected Papers of Hirotugu Akaike, Springer
Series in Statistics, pages 199–213. Springer, New York, NY, 1998.

Clifford M. Hurvich and Chih-Ling Tsai. Regression and time series model
selection in small samples. June 1989.

Hurvich Clifford M., Simonoff Jeffrey S., and Tsai Chih-Ling. Smoothing
parameter selection in nonparametric regression using an improved Akaike
information criterion. January 2002.
\end{References}
\printindex{}
\end{document}
