---
title: "Using the CVEK R package"
author: "Wenying Deng"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Using the CVEK R package}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

## Short Description

Using a library of base kernels, **CVEK** learns a proper generating function from data by directly minimizing the ensemble model’s error, and tests whether the data is generated by the RKHS under the null hypothesis. Part I presents a simple example to conduct Gaussian process regression and hypothesis testing using  *cvek* function on simulated data. And Part II shows a real-world application where we use **CVEK** to test for *longley* dataset.

Download the package from CRAN or [GitHub](https://github.com/IrisTeng/CVEK-1) and then install and load it.

```{r, message = FALSE}
# devtools::install_github("IrisTeng/CVEK-1")
library(CVEK)
```


## Tutorial using simulation dataset

### Generate Data and Define Model

We generate a simulation dataset using $"rbf"$ kernel with length-scale $1$, and set the relative interaction strength to be $0.2$. The outcome $y_i$ is generated as,
\begin{align*}
y_i=h_1(\mathbf{x}_{i, 1})+h_2(\mathbf{x}_{i, 2})+0.2 * h_{12}(\mathbf{x}_{i, 1}, \mathbf{x}_{i, 2})+\epsilon_i,
\end{align*}
where $h_1$, $h_2$, $h_{12}$ are sampled from RKHSs $\textit{H}_1$, $\textit{H}_2$, $\textit{H}_{12}$, generated using the corresponding *rbf* kernel. We standardize all sampled functions to have unit form, so that $0.2$ represents the strength of interaction relative to the main effect.

```{r}
set.seed(0726)
n <- 150 # including training and test
d <- 4
int_effect <- 0.2
data <- matrix(rnorm(n * d), ncol = d)
Z1 <- data[, 1:2]
Z2 <- data[, 3:4]

kern <- generate_kernel(method = "rbf", l = 1)
w <- rnorm(n)
w12 <- rnorm(n)
K1 <- kern(Z1, Z1)
K2 <- kern(Z2, Z2)
K1 <- K1 / sum(diag(K1)) # standardize kernel
K2 <- K2 / sum(diag(K2))
h0 <- K1 %*% w + K2 %*% w
h0 <- h0 / sqrt(sum(h0 ^ 2)) # standardize main effect

h1_prime <- (K1 * K2) %*% w12 # interaction effect

# standardize sampled functions to have unit norm, so that 0.2
# represents the interaction strength relative to main effect
Ks <- svd(K1 + K2)
len <- length(Ks$d[Ks$d / sum(Ks$d) > .001])
U0 <- Ks$u[, 1:len]
h1_prime_hat <- fitted(lm(h1_prime ~ U0))
h1 <- h1_prime - h1_prime_hat

h1 <- h1 / sqrt(sum(h1 ^ 2)) # standardize interaction effect
Y <- h0 + int_effect * h1 + rnorm(1) + rnorm(n, 0, 0.01)
data <- as.data.frame(cbind(Y, Z1, Z2))
colnames(data) <- c("y", paste0("z", 1:d))

data_train <- data[1:100, ]
data_test <- data[101:150, ]
```


```{r, results='asis'}
knitr::kable(head(data_train, 5))
```


We can apply *cvek* function to conduct Gaussian process regression.

Below is a detailed list of all the arguments of the function *cvek*.
```{r, fig.width=14, fig.height=11}
knitr::include_graphics("table1.pdf", auto_pdf = TRUE)
```


Suppose we want our kernel library to contain three kernels: $"linear"$, $"polynomial"$ with $p=2$, and $"rbf"$ with $l=1$ (the effective parameter for $"polynomial"$ is $p$ and the effective 
parameter for $"rbf"$ is $l$, so we can set anything to $l$ for $"polynomial"$ kernel and $p$ for
$"rbf"$ kernel), apply *define_library*.

```{r}
kern_par <- data.frame(method = c("linear", "polynomial", "rbf"), 
                       l = rep(1, 3), p = 1:3, stringsAsFactors = FALSE)
# define kernel library
kern_func_list <- define_library(kern_par)
```

And the null model is $y \sim z1 + z2 + k(z3, z4)$.
```{r}
formula <- y ~ z1 + z2 + k(z3, z4)
```



### Estimation and Testing

With all these parameters specified, we can conduct Gaussian process regression.

```{r}
est_res <- cvek(formula, kern_func_list = kern_func_list, data = data_train)
est_res$lambda
est_res$u_hat
```

We can see that there are three elements in $beta$ with the first one as the intercept. More
importantly, the ensemble weight assigns $1$ to the $"rbf"$ kernel, which is the true kernel. 
This illustrates the accuracy and efficiency of the CVEK method.

Then, here comes the testing procedure. 

Note that we can use the same function *cvek* as estimation to perform hypothesis testing, but we need to provide $formula\_test$, which is the user-supplied formula indicating the alternative 
effect to test.

Now, we want to conduct score test with $test="boot"$ and $B=200$ since the sample $n=100$ is small.

```{r}
formula_test <- y ~ k(z1, z2):k(z3, z4)
pvalue <- cvek(formula, kern_func_list = kern_func_list, 
               data = data_train, formula_test = formula_test,
               mode = "loocv", strategy = "stack",
               beta_exp = 1, lambda = exp(seq(-10, 5)),
               test = "boot", alt_kernel_type = "linear",
               B = 200, verbose = FALSE)$pvalue
pvalue
```

So at the significance level $0.05$, we reject the null hypothesis that there's no interaction effect, which matches our data generation mechanism.


Additionally, we can prediction new outcomes based on estimation result *est_res*.

```{r}
y_pred <- predict(est_res, data_test[, 2:5])
data_test_pred <- cbind(y_pred, data_test)
```


```{r, echo=FALSE, results='asis'}
knitr::kable(head(data_test_pred, 5))
```


## Tutorial using *longley* dataset

For real dataset, we use *longley* dataset that is built into R. *longley* is a macroeconomic dataset which provides a well-known example for a highly collinear regression. It is a data frame with 7 economical variables, observed yearly from 1947 to 1962. The regression *lm(Employed ~ .)* is known to be highly collinear. Here the null model is,
\begin{align*}
Employed \sim Population + k(Armed.Forces, Population),
\end{align*}
where $k()$ is the estimated ensemble kernel matrix, with weights estimated using *cvek*. We are testing whether there's interaction effect between $Armed.Forces$ and $Population$. Therefore,
\begin{align*}
\textit{H}_0: &\; Employed \sim Population + k(Armed.Forces, Population), \\
\textit{H}_a: &\; Employed \sim Population + k(Armed.Forces, Population)\\
& \qquad \qquad \qquad \qquad \qquad \qquad \qquad+ k(Armed.Forces):k(Population).
\end{align*}

```{r}
kern_par <- data.frame(method = c("linear", "polynomial", "rbf"), 
                       l = rep(1, 3), p = 1:3, stringsAsFactors = FALSE)
# define kernel library
kern_func_list <- define_library(kern_par)

formula <- Employed ~ Population + k(Armed.Forces, Population)
formula_test <- Employed ~ k(Armed.Forces):k(Population)
fit_longley <- cvek(formula, kern_func_list = kern_func_list, data = longley, 
                    formula_test = formula_test)
fit_longley$pvalue
```
Here we use the same kernel library as the one in the simulation dataset: $"linear"$, $"polynomial"$ with $p=2$, and $"rbf"$ with $l=1$. In this case, we fail to reject the null hypothesis that there's no interaction effect. 


## References

1.  Jeremiah Zhe Liu and Brent Coull. Robust Hypothesis Test for Nonlinear Effect       with Gaussian Processes. October 2017.
1.  Xiang Zhan, Anna Plantinga, Ni Zhao, and Michael C. Wu. A fast small-sample         kernel inde- pendence test for microbiome community-level association analysis.     December 2017.
1.  Arnak S. Dalalyan and Alexandre B. Tsybakov. Aggregation by Exponential             Weighting and Sharp Oracle Inequalities. In Learning Theory, Lecture Notes in       Computer Science, pages 97– 111. Springer, Berlin, Heidelberg, June 2007.
1.  Arnab Maity and Xihong Lin. Powerful tests for detecting a gene effect in the       presence of possible gene-gene interactions using garrote kernel machines.          December 2011.
1.  The MIT Press. Gaussian Processes for Machine Learning, 2006.
1.  Xihong Lin. Variance component testing in generalised linear models with random     effects. June 1997.
1.  Philip S. Boonstra, Bhramar Mukherjee, and Jeremy M. G. Taylor. A Small-Sample      Choice of the Tuning Parameter in Ridge Regression. July 2015.
1.  Trevor Hastie, Robert Tibshirani, and Jerome Friedman. The Elements of              Statistical Learning: Data Mining, Inference, and Prediction, Second Edition.       Springer Series in Statistics. Springer- Verlag, New York, 2 edition, 2009.
1.  Hirotogu Akaike. Information Theory and an Extension of the Maximum Likelihood      Princi- ple. In Selected Papers of Hirotugu Akaike, Springer Series in              Statistics, pages 199–213. Springer, New York, NY, 1998.
1.  Clifford M. Hurvich and Chih-Ling Tsai. Regression and time series model            selection in small samples. June 1989.
1.  Hurvich Clifford M., Simonoff Jeffrey S., and Tsai Chih-Ling. Smoothing             parameter selection in nonparametric regression using an improved Akaike            information criterion. January 2002.
